{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi modal 3D brain tumor segmentaion using the BRATS 2016-2017 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install monai\n",
    "MONAI is a PyTorch-based, open-source framework for deep learning in healthcare imaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install monai\n",
    "# !pip install 'monai[all]'\n",
    "# !python -c \"import monai\" || pip install -q \"monai-weekly[nibabel, tqdm]\"\n",
    "# !pip install nilearn\n",
    "# !pip install git+https://github.com/miykael/gif_your_nifti"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import relevant libraries and functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import monai\n",
    "import matplotlib\n",
    "import torch\n",
    "import nilearn as nl\n",
    "import nilearn.plotting as nlplt\n",
    "import nibabel as nib\n",
    "import gif_your_nifti.core as gif2nif\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from skimage.io import imshow\n",
    "from skimage.color import label2rgb\n",
    "from skimage.util import montage \n",
    "from skimage.transform import rotate\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, Dataset\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import UNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsChannelFirstd,\n",
    "    AsDiscrete,\n",
    "    CenterSpatialCropd,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Spacingd,\n",
    "    ToTensord,\n",
    ")\n",
    "from monai.utils import first, set_determinism\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed\n",
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create helper functions to read, split and process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvertToMultiChannelBasedOnBratsClassesd(MapTransform):\n",
    "    \"\"\"\n",
    "    Convert labels to multi channels based on brats classes:\n",
    "    label 0 is no tumor\n",
    "    label 1 is the edema\n",
    "    label 2 is the non enhancing tumor\n",
    "    label 3 is enhancing tumor core\n",
    "    The final classes are TC (Tumor core), WT (Whole tumor)\n",
    "    and ET (Enhancing tumor).\n",
    "\n",
    "    \"\"\"\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            # merge label 2 and label 3 to construct TC\n",
    "            result.append(np.logical_or(d[key] == 2, d[key] == 3))\n",
    "            # merge labels 1, 2 and 3 to construct WT\n",
    "            result.append(\n",
    "                np.logical_or(\n",
    "                    np.logical_or(d[key] == 2, d[key] == 3), d[key] == 1)\n",
    "            )\n",
    "            # label 3 is ET\n",
    "            result.append(d[key] == 2)\n",
    "            d[key] = np.stack(result, axis=0).astype(np.float32)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transforms():\n",
    "    '''\n",
    "    The function creates the tranformations to be applied to the training and validation sets\n",
    "    \n",
    "    input: \n",
    "        no arguments\n",
    "    output: \n",
    "        training and validation transforms (pytorch transforms)\n",
    "    '''\n",
    "    train_transforms = Compose(\n",
    "        [\n",
    "            # load 4 Nifti images and stack them together\n",
    "            LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            AsChannelFirstd(keys=\"image\"),\n",
    "            ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "            Spacingd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                pixdim=(1.5, 1.5, 2.0),\n",
    "                mode=(\"bilinear\", \"nearest\"),\n",
    "            ),\n",
    "            Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "            RandSpatialCropd(\n",
    "                keys=[\"image\", \"label\"], roi_size=[128, 128, 64], random_size=False\n",
    "            ),\n",
    "            RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "            NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "            RandScaleIntensityd(keys=\"image\", factors=0.1, prob=0.5),\n",
    "            RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=0.5),\n",
    "            ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "    )\n",
    "    val_transforms = Compose(\n",
    "        [\n",
    "            LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            AsChannelFirstd(keys=\"image\"),\n",
    "            ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "            Spacingd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                pixdim=(1.5, 1.5, 2.0),\n",
    "                mode=(\"bilinear\", \"nearest\"),\n",
    "            ),\n",
    "            Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "            CenterSpatialCropd(keys=[\"image\", \"label\"], roi_size=[128, 128, 64]),\n",
    "            NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "            ToTensord(keys=[\"image\", \"label\"]),\n",
    "        ]\n",
    "    )\n",
    "    return train_transforms, val_transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data_dir):\n",
    "    '''\n",
    "    The function reads and splits the dataset into train and validation set\n",
    "    \n",
    "    input: \n",
    "        data_dir: the path of the directory where data exists (type: str)\n",
    "    output: \n",
    "        list of {image name : label name} pairs for train and validation sets (type: list of dictionaries)\n",
    "    '''\n",
    "    # get list of image and labels list\n",
    "    train_images = sorted(glob.glob(os.path.join(data_dir, \"imagesTr\", \"*.nii.gz\")))\n",
    "    train_labels = sorted(glob.glob(os.path.join(data_dir, \"labelsTr\", \"*.nii.gz\")))\n",
    "    # create a dictionary of the images and labels pairs\n",
    "    data_dicts = [{\"image\": image_name, \"label\": label_name} for image_name, label_name in zip(train_images, train_labels)]\n",
    "    # split the training set into train and set sets\n",
    "    train_idx, val_idx = train_test_split(np.arange(len(train_images)), test_size=0.2)\n",
    "    train_files, val_files = list(np.array(data_dicts)[train_idx]) , list(np.array(data_dicts)[val_idx])\n",
    "    return train_files, val_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the dataset and dataloaders using the helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/'\n",
    "train_files, val_files = split_dataset(data_dir)\n",
    "train_transforms , val_transforms = _transforms()\n",
    "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=0)\n",
    "\n",
    "val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_images_and_labels(dataset,image_id,image_slice):\n",
    "    '''\n",
    "    The function visualises the 4 modalities (T2 FAIR, T1, T1Gd and T2)\n",
    "    along with the three output segmentaion ground truths (Whole tumor, Tumor core and Enhancing tumor)\n",
    "    input:\n",
    "        \n",
    "    '''\n",
    "    plt.figure(\"image\", (24, 6))\n",
    "    print(f\"image shape: {dataset[image_id]['image'].shape}\")\n",
    "    modes = ['T2 FLAIR','T1','T1Gd','T2']\n",
    "    for i in range(4):\n",
    "        plt.subplot(1, 4, i + 1)\n",
    "        plt.title(f\"{modes[i]}\")\n",
    "        plt.imshow(dataset[image_id][\"image\"][i, :, :, image_slice].detach().cpu(), cmap=\"gray\")\n",
    "    plt.show()\n",
    "    # also visualize the 3 channels label corresponding to this image\n",
    "    print(f\"label shape: {dataset[image_id]['label'].shape}\")\n",
    "    plt.figure(\"label\", (18, 6))\n",
    "    labels = ['Tumor core', 'Whole tumor' , 'Enhancing tumor']\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i + 1)\n",
    "        plt.title(f\"{labels[i]}\")\n",
    "        plt.imshow(dataset[image_id][\"label\"][i, :, :, image_slice].detach().cpu(), cmap =\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualise_images_and_labels(val_ds,7,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise all slices of T2 FLAIR \n",
    "fig, ax1 = plt.subplots(1, 1, figsize = (15,15))\n",
    "ax1.imshow(rotate(montage(val_ds[7][\"image\"][0,:,:,].detach().cpu()), 90, resize=True), cmap ='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_gif(dataset,image_id,out_location, modality = 0):\n",
    "    '''\n",
    "    saves the gif depicting all the slices of a specified madality\n",
    "    input:\n",
    "        dataset: the pytorch dataset object\n",
    "        image_id: the index of the image in the dataset\n",
    "        out_location: the location and name of the output file\n",
    "        modality: the index of the particular modality in the dataset\n",
    "        \n",
    "    output: \n",
    "        an nii image of the modality selected\n",
    "        a gif image of the  modality selected\n",
    "    '''\n",
    "    img = dataset[image_id][\"image\"][0,:,:,:].detach().cpu().numpy()\n",
    "    lab = dataset[image_id][\"label\"][0,:,:,:].detach().cpu().numpy()\n",
    "    img = nib.Nifti1Image(img, np.eye(4))\n",
    "    lab = nib.Nifti1Image(lab, np.eye(4))\n",
    "    nib.save(img, out_location+'/test.nii')\n",
    "    nib.save(lab, out_location+'/test_lab.nii')\n",
    "    gif2nif.write_gif_normal(out_location+'/test.nii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_gif(val_ds, 7, 'data/samples/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![flair_gif](data/samples/test.gif \"hi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the 3D UNET model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the multi-modal multi output 3D-unet model using monai \n",
    "device = torch.device(\"cuda:0\")\n",
    "model = UNet(\n",
    "    dimensions=3,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    channels=(16, 32, 64, 128, 256),\n",
    "    strides=(2, 2, 2, 2),\n",
    "    num_res_units=2,\n",
    ").to(device)\n",
    "\n",
    "loss_function = DiceLoss(to_onehot_y=False, sigmoid=True, squared_pred=True)\n",
    "\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(), 1e-4, weight_decay=1e-5, amsgrad=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 200\n",
    "val_interval = 2\n",
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "        )\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "            post_trans = Compose(\n",
    "                [Activations(sigmoid=True), AsDiscrete(threshold_values=True)]\n",
    "            )\n",
    "            metric_sum = metric_sum_tc = metric_sum_wt = metric_sum_et = 0.0\n",
    "            metric_count = (\n",
    "                metric_count_tc\n",
    "            ) = metric_count_wt = metric_count_et = 0\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_outputs = post_trans(val_outputs)\n",
    "                # compute overall mean dice\n",
    "                value, not_nans = dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                not_nans = not_nans.item()\n",
    "                metric_count += not_nans\n",
    "                metric_sum += value.item() * not_nans\n",
    "                # compute mean dice for TC\n",
    "                value_tc, not_nans = dice_metric(\n",
    "                    y_pred=val_outputs[:, 0:1], y=val_labels[:, 0:1]\n",
    "                )\n",
    "                not_nans = not_nans.item()\n",
    "                metric_count_tc += not_nans\n",
    "                metric_sum_tc += value_tc.item() * not_nans\n",
    "                # compute mean dice for WT\n",
    "                value_wt, not_nans = dice_metric(\n",
    "                    y_pred=val_outputs[:, 1:2], y=val_labels[:, 1:2]\n",
    "                )\n",
    "                not_nans = not_nans.item()\n",
    "                metric_count_wt += not_nans\n",
    "                metric_sum_wt += value_wt.item() * not_nans\n",
    "                # compute mean dice for ET\n",
    "                value_et, not_nans = dice_metric(\n",
    "                    y_pred=val_outputs[:, 2:3], y=val_labels[:, 2:3]\n",
    "                )\n",
    "                not_nans = not_nans.item()\n",
    "                metric_count_et += not_nans\n",
    "                metric_sum_et += value_et.item() * not_nans\n",
    "                \n",
    "            metric = metric_sum / metric_count\n",
    "            metric_values.append(metric)\n",
    "            metric_tc = metric_sum_tc / metric_count_tc\n",
    "            metric_values_tc.append(metric_tc)\n",
    "            metric_wt = metric_sum_wt / metric_count_wt\n",
    "            metric_values_wt.append(metric_wt)\n",
    "            metric_et = metric_sum_et / metric_count_et\n",
    "            metric_values_et.append(metric_et)\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(root_dir, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the loss and the dice score per epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(epoch_loss_values))]\n",
    "y = epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values))]\n",
    "y = metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_tc))]\n",
    "y = metric_values_tc\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_wt))]\n",
    "y = metric_values_wt\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "x = [val_interval * (i + 1) for i in range(len(metric_values_et))]\n",
    "y = metric_values_et\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the model results versus the ground truth "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_results(dataset,model_path,image_id,image_slice):\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "    post_trans = Compose(\n",
    "                [Activations(sigmoid=True), AsDiscrete(threshold_values=True)]\n",
    "            )\n",
    "    with torch.no_grad():\n",
    "        # select one image to evaluate and visualize the model output\n",
    "        val_input = dataset[image_id][\"image\"].unsqueeze(0).to(device)\n",
    "        val_output = model(val_input)\n",
    "        val_output = post_trans(val_output)\n",
    "        plt.figure(\"image\", (24, 6))\n",
    "        modes = ['T2 FLAIR','T1','T1Gd','T2']\n",
    "        print('Image modalities')\n",
    "        for i in range(4):\n",
    "            plt.subplot(1, 4, i + 1)\n",
    "            plt.title(f\"{modes[i]}\")\n",
    "            plt.imshow(dataset[image_id][\"image\"][i, :, :, image_slice].detach().cpu(), cmap=\"gray\")\n",
    "        plt.show()\n",
    "        \n",
    "        # visualize the 3 channels label corresponding to this image\n",
    "        plt.figure(\"label\", (18, 6))\n",
    "        labels = ['Tumor core', 'Whole tumor' , 'Enhancing tumor']\n",
    "        print('Ground truth masks')\n",
    "        for i in range(3):\n",
    "            plt.subplot(1, 3, i + 1)\n",
    "            plt.title(f\"{labels[i]}\")\n",
    "            plt.imshow(dataset[image_id][\"label\"][i, :, :, image_slice].detach().cpu(), cmap =\"gray\")\n",
    "        plt.show()\n",
    "        # visualize the 3 channels model output corresponding to this image\n",
    "        plt.figure(\"output\", (18, 6))\n",
    "        print('Predicted masks')\n",
    "        for i in range(3):\n",
    "            plt.subplot(1, 3, i + 1)\n",
    "            plt.title(f\"{labels[i]}\")\n",
    "            plt.imshow(val_output[0, i, :, :, image_slice].detach().cpu(), cmap =\"gray\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"best_metric_model.pth\")\n",
    "visualise_results(val_ds,model_path,3,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
